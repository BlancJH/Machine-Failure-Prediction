{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Failure Prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.Data Loading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1 Module Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.modules.outlier_detector import detect_outliers\n",
    "from src.modules.outlier_imputer import OutlierImputer\n",
    "from src.modules.feature_scaler import preprocessor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBRegressor\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, KFold\n",
    "from sklearn.metrics import mean_absolute_percentage_error, r2_score, make_scorer\n",
    "from src.modules.time_extractor import TimeFeatureExtractor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"data/IndFD-PM-DT dataset.csv\")\n",
    "\n",
    "# Verify data load\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect dataframe shape\n",
    "print(\"Dataset shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types of each column and non-null value count\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing values\n",
    "missing_counts = df.isnull().sum()\n",
    "print(\"Missing values per column:\\n\", missing_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count duplicates\n",
    "duplicate_rows = df.duplicated().sum()\n",
    "print(\"Number of duplicate rows:\", duplicate_rows)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load splited data\n",
    "\n",
    "X = df.drop(\"Fault_Diagnosis\", axis=1)\n",
    "y = df[\"Fault_Diagnosis\"]\n",
    "\n",
    "train_idx = np.load(\"data/train_idx.npy\")\n",
    "test_idx  = np.load(\"data/test_idx.npy\")\n",
    "\n",
    "X_train = X.iloc[train_idx]\n",
    "X_test  = X.iloc[test_idx]\n",
    "y_train = y.iloc[train_idx]\n",
    "y_test  = y.iloc[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set preprocessing pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('outlier_imputer', OutlierImputer(multiplier=1.5)),\n",
    "    ('time_extractor', TimeFeatureExtractor(column='Datetime')),\n",
    "    ('scaler', preprocessor)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform datasets\n",
    "preprocessor = pipeline.fit(X_train)\n",
    "\n",
    "X_train_proc = preprocessor.transform(X_train)\n",
    "X_test_proc = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the evaluation metrics\n",
    "mape_scorer = make_scorer(mean_absolute_percentage_error, greater_is_better=False)\n",
    "r2_scorer = make_scorer(r2_score, greater_is_better=True)\n",
    "\n",
    "scoring = {\n",
    "    \"MAPE\": mape_scorer,\n",
    "    \"R2\": r2_scorer\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost pipeline\n",
    "xgb_pipeline = Pipeline([\n",
    "    (\"preproccess_pipeline\", pipeline),\n",
    "    (\"regressor\", XGBRegressor(objective='reg:squarederror', random_state=42))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter search range\n",
    "param_dist = {\n",
    "    # number of trees\n",
    "    \"regressor__n_estimators\": randint(500, 1200),\n",
    "\n",
    "    # tree depth\n",
    "    \"regressor__max_depth\": randint(4, 10),\n",
    "\n",
    "    # step size shrinkage\n",
    "    \"regressor__learning_rate\": uniform(0.005, 0.095),\n",
    "\n",
    "    # subsample ratio of the training instance\n",
    "    \"regressor__subsample\": uniform(0.7, 0.3),\n",
    "\n",
    "    # subsample ratio of columns when constructing each tree\n",
    "    \"regressor__colsample_bytree\": uniform(0.7, 0.3),\n",
    "\n",
    "    # L1 & L2 regularisation weights\n",
    "    \"regressor__reg_alpha\": uniform(0.0, 1.0),\n",
    "    \"regressor__reg_lambda\": uniform(0.5, 1.5),\n",
    "\n",
    "    # minimum sum of instance weight needed\n",
    "    \"regressor__min_child_weight\": randint(1, 10),\n",
    "\n",
    "    # minimum loss reduction\n",
    "    \"regressor__gamma\": uniform(0.0, 0.5),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set randomised Search Cross validation\n",
    "search = RandomizedSearchCV(\n",
    "    xgb_pipeline,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=50, # try 50 random combos\n",
    "    scoring=scoring,\n",
    "    refit=\"MAPE\", # Set MAPE as the optimiser metric.\n",
    "    cv=KFold(n_splits=5, shuffle=True, random_state=42),\n",
    "    random_state=42,\n",
    "    return_train_score=True,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and the score\n",
    "print(\"Best params:\", search.best_params_)\n",
    "print(\"Best CV MAPE:\", -search.best_score_)\n",
    "print(\"Corresponding CV R2:\", search.cv_results_[\"mean_test_R2\"][search.best_index_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test data\n",
    "\n",
    "# Find the best model\n",
    "best_model = search.best_estimator_\n",
    "\n",
    "# Set validation target\n",
    "y_pred_test = best_model.predict(X_test)\n",
    "\n",
    "# Calculate the evaluation\n",
    "hold_mape = mean_absolute_percentage_error(y_test, y_pred_test)\n",
    "hold_r2 = r2_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Hold‑out MAPE: {hold_mape:.4f}\")\n",
    "print(f\"Hold‑out R²: {hold_r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
